{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-09T22:23:31.288906900Z",
     "start_time": "2023-11-09T22:23:18.160870600Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T22:23:40.416991600Z",
     "start_time": "2023-11-09T22:23:31.304963800Z"
    }
   },
   "id": "f99357b1fc38a70f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T22:27:24.193154300Z",
     "start_time": "2023-11-09T22:27:24.180598900Z"
    }
   },
   "id": "6de4974f2d2977d9"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def responder(pregunta):\n",
    "    input = tokenizer.encode(pregunta,return_tensors=\"tf\")\n",
    "    modelg = model.generate(input,max_length=50,num_beams = 5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "    outputD= tokenizer.decode(modelg[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return outputD"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T00:03:34.284499600Z",
     "start_time": "2023-11-09T00:03:34.252115900Z"
    }
   },
   "id": "51153d0e044d1170"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'Where is United Estates located?\\n\\nUnited Estations is located in the heart of New York City. It is one of the oldest and most well-known residential and commercial real estate developments on the East Coast. The building was built in 1892 and is now the largest residential building in North America.\\n\\n\\nWhat is the difference between the building and the other buildings on this site? What are the differences between these two buildings? Are there any differences in how they are constructed? How does'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responder(\"Where is United Estates\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T00:08:24.510370800Z",
     "start_time": "2023-11-09T00:07:14.922831200Z"
    }
   },
   "id": "4102c46812010a23"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'Are Machine intelligent than human being?\\n\\nThe answer to this question is a resounding \"yes\". The problem is that machine intelligence is not the same as human intelligence. Machine intelligence does not know what is going on in the world, but it does know that it is doing something. It knows what it wants to do, and it will do it. This is what makes it so powerful. If you want to make a machine intelligent, you have to be able to understand what you are doing'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responder(\"Are Machine intelligent than human being\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T00:14:15.605980900Z",
     "start_time": "2023-11-09T00:12:34.391249Z"
    }
   },
   "id": "83a4222effd66bce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4d3c2951d32b002"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "data = [\"Dónde está Berlín? <SEP> Se encuentra en Sucre, Bolivia, a 3 cuadras de la plaza principal\",\n",
    "\"¿Cuál es la ubicación de Berlín? <SEP> Berlín se ubica en Sucre, Bolivia, a unas tres cuadras de la plaza principal\",\n",
    "\"En qué lugar está Berlín? <SEP> Está en el centro de Sucre, Bolivia, a una corta distancia de la plaza principal\",\n",
    "\"¿Dónde se halla Berlín? <SEP> Se halla en Sucre, Bolivia, a 3 cuadras de la plaza principal.\",\n",
    "\"¿Dónde se localiza Berlín? <SEP> Se localiza en Sucre, Bolivia, a unas pocas cuadras de la plaza principal.\",\n",
    "\"Dónde se encuentra Berlín <SEP> Se encuentra en Sucre, Bolivia, a tres cuadras de la plaza principal\",\n",
    "\"¿En qué parte de Sucre está Berlín? <SEP> Berlín está en el corazón de Sucre, Bolivia, a corta distancia de la plaza principal.\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T15:01:34.175670300Z",
     "start_time": "2023-11-15T15:01:34.164420500Z"
    }
   },
   "id": "837b69468fb1f2ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, datos, tokenizer, max_length=128):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        for i in datos:\n",
    "            question, answer = i.split('[SEP]')\n",
    "            self.data.append((question,answer))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        question, answer = self.data[index]\n",
    "        # Tokenizar y codificar las secuencias\n",
    "        input_ids = self.tokenizer.encode(question, add_special_tokens=True, max_length=self.max_length, truncation=True)\n",
    "        target_ids = self.tokenizer.encode(answer, add_special_tokens=True, max_length=self.max_length, truncation=True)\n",
    "\n",
    "        # Convertir a tensores de PyTorch\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        target_ids = torch.tensor(target_ids, dtype=torch.long)\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"target_ids\": target_ids}\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# Supongamos que tienes un modelo tokenizador llamado 'tokenizer' y un archivo de datos llamado 'chatbot_data.txt'\n",
    "tokenizer = YourTokenizer()  # Reemplaza YourTokenizer con el objeto real de tu tokenizador\n",
    "dataset = ChatbotDataset(data_path='chatbot_data.txt', tokenizer=tokenizer)\n",
    "\n",
    "# Divide el conjunto de datos en entrenamiento y validación (80-20 por ejemplo)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Crea los dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "941dd9ce021f42c3"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['abc ', ' kjd']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = \"abc <sep> kjd\"\n",
    "vector.strip().split('<sep>')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T15:03:41.098713500Z",
     "start_time": "2023-11-15T15:03:41.066424900Z"
    }
   },
   "id": "26ef9221923c19a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9aaee187cffd524"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9925295e8520528f"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[   35, 10205,   358,    68,  1556,  6557,  4312,    75, 39588,    30,\n          1001,  2207,    84,   298,   430,   551, 47352,   260,    11, 38496,\n            11,   257,   513, 18912,   324,  8847,   390,  8591, 42433, 10033,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256],\n        [  126,   123, 46141,  6557,    75,  1658,  8591, 20967,   291, 32009,\n         18840,   390,  4312,    75, 39588,    30,  4312,    75, 39588,   384,\n         20967,  3970,   551, 47352,   260,    11, 38496,    11,   257,   555,\n           292,   256,   411, 18912,   324,  8847,   390,  8591, 42433, 10033,\n         50256, 50256, 50256],\n        [ 4834,   627,  2634,   300, 35652,  1556,  6557,  4312,    75, 39588,\n            30, 10062,  6557,   551,  1288,  1247,   305,   390, 47352,   260,\n            11, 38496,    11,   257,   555,    64, 12794,    64,  1233,  1192,\n           544,   390,  8591, 42433, 10033, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256],\n        [  126,   123,    35, 10205,   358,    68,   384,  6899,    64,  4312,\n            75, 39588,    30,  1001,  6899,    64,   551, 47352,   260,    11,\n         38496,    11,   257,   513, 18912,   324,  8847,   390,  8591, 42433,\n         10033,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256],\n        [  126,   123,    35, 10205,   358,    68,   384,  1957, 23638,  4312,\n            75, 39588,    30,  1001,  1957, 23638,   551, 47352,   260,    11,\n         38496,    11,   257,   555,   292,   279,   420,   292, 18912,   324,\n          8847,   390,  8591, 42433, 10033,    13, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256],\n        [   35, 10205,   358,    68,   384,  2207,    84,   298,   430,  4312,\n            75, 39588,  1001,  2207,    84,   298,   430,   551, 47352,   260,\n            11, 38496,    11,   257,   256,   411, 18912,   324,  8847,   390,\n          8591, 42433, 10033, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256],\n        [  126,   123,  4834,   627,  2634,   636,    68,   390, 47352,   260,\n          1556,  6557,  4312,    75, 39588,    30,  4312,    75, 39588,  1556,\n          6557,   551,  1288,  1162,  1031, 18840,   390, 47352,   260,    11,\n         38496,    11,   257, 12794,    64,  1233,  1192,   544,   390,  8591,\n         42433, 10033,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = tokenizer(data, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "tokenized_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T22:27:35.913153200Z",
     "start_time": "2023-11-09T22:27:35.796864100Z"
    }
   },
   "id": "5735325c4d3a87ae"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class MyTextDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.input_ids = tokenized_data[\"input_ids\"]\n",
    "        self.attention_mask = tokenized_data[\"attention_mask\"]\n",
    "        self.label = tokenized_data[\"input_ids\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"label\":self.input_ids[idx]\n",
    "        }\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T13:09:18.235873800Z",
     "start_time": "2023-11-10T13:09:18.215563600Z"
    }
   },
   "id": "3d71d80c0f24c015"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "embed_dim = 2560\n",
    "num_heads = 12\n",
    "\n",
    "# Ajustar embed_dim para que sea divisible uniformemente por num_heads\n",
    "if embed_dim % num_heads != 0:\n",
    "    embed_dim = (embed_dim // num_heads) * num_heads\n",
    "\n",
    "# Configurar el modelo con los nuevos valores\n",
    "config = GPT2Config(embed_dim=embed_dim, num_heads=num_heads)\n",
    "# Cargar el modelo preentrenado y el tokenizador\n",
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "modelPytorch = GPT2LMHeadModel.from_pretrained(model_name, config=config)\n",
    "tokenizerPytorch = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizerPytorch.pad_token = tokenizerPytorch.eos_token"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-10T14:42:37.402813400Z"
    }
   },
   "id": "af6f1548912417a4"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "\n",
    "tokenized_dataPytorch = tokenizerPytorch(data, return_tensors=\"pt\", padding=True, truncation=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T14:22:34.572290200Z",
     "start_time": "2023-11-10T14:22:34.528977800Z"
    }
   },
   "id": "769c6f3f2e97777a"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "text_dataset = MyTextDataset(tokenized_dataPytorch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T14:22:35.403142700Z",
     "start_time": "2023-11-10T14:22:35.396424600Z"
    }
   },
   "id": "743e37bdaf4c002c"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizerPytorch, mlm=False)\n",
    "train_dataloader = DataLoader(text_dataset, collate_fn=data_collator, batch_size=2, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T14:22:35.936982700Z",
     "start_time": "2023-11-10T14:22:35.920718800Z"
    }
   },
   "id": "b00bbe0e0a6c33b3"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T14:22:36.417640400Z",
     "start_time": "2023-11-10T14:22:36.413128400Z"
    }
   },
   "id": "b875714edc0bd73f"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    inputs = batch[\"input_ids\"]\n",
    "    #labels = batch[\"labels\"]\n",
    "    outputs = modelPytorch(inputs, labels=inputs)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T14:22:39.605213700Z",
     "start_time": "2023-11-10T14:22:36.792979500Z"
    }
   },
   "id": "6bfd24d0f367f588"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "import torch\n",
    "def responder(pregunta):\n",
    "    input = tokenizerPytorch.encode(pregunta,return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones_like(input)\n",
    "    if attention_mask.any():\n",
    "        modelPy = modelPytorch.generate(input, attention_mask= attention_mask,max_length=50,num_beams = 5, no_repeat_ngram_size=2, early_stopping=True, pad_token_id=tokenizerPytorch.eos_token_id)\n",
    "        outputD= tokenizerPytorch.decode(modelPy[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        return outputD\n",
    "    return \"Mascara vacia\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T14:22:41.870069900Z",
     "start_time": "2023-11-10T14:22:41.860078100Z"
    }
   },
   "id": "e1e46a5d9e3c3944"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "\"En qué lugar está Berlín?\\n\\nNo, I don't think so. I mean, you know, it's not like we're going to be able to do anything about it. It's just that we have to\""
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responder(\"En qué lugar está Berlín?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T14:22:45.654762900Z",
     "start_time": "2023-11-10T14:22:42.753359100Z"
    }
   },
   "id": "6cef6eb3a49bdf34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6196422fb9f5bae2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5dab672f80d16a95"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1cd03ba7f079e818"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7cf46ee9506939f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cebf2c465ceeec4a"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextDataset.__init__() got an unexpected keyword argument 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m text_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mTextDataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfile_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Puedes proporcionar un archivo si tus datos están almacenados en un archivo en lugar de una lista en memoria.\u001B[39;49;00m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mblock_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Tamaño del bloque para la generación de texto\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43moverwrite_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Si debes sobrescribir la caché existente\u001B[39;49;00m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtokenized_data\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: TextDataset.__init__() got an unexpected keyword argument 'input_ids'"
     ]
    }
   ],
   "source": [
    "text_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=None,  # Puedes proporcionar un archivo si tus datos están almacenados en un archivo en lugar de una lista en memoria.\n",
    "    block_size=128,  # Tamaño del bloque para la generación de texto\n",
    "    overwrite_cache=False,  # Si debes sobrescribir la caché existente\n",
    "    **tokenized_data\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T22:28:41.232438300Z",
     "start_time": "2023-11-09T22:28:41.172745300Z"
    }
   },
   "id": "f7f92d93f15134f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "toke"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6801f88190311d3"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "[35,\n 10205,\n 358,\n 68,\n 1556,\n 6557,\n 4312,\n 75,\n 39588,\n 30,\n 1001,\n 2207,\n 84,\n 298,\n 430,\n 551,\n 47352,\n 260,\n 11,\n 38496,\n 11,\n 257,\n 513,\n 18912,\n 324,\n 8847,\n 390,\n 8591,\n 42433,\n 10033]"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenizer(data)\n",
    "input.input_ids[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T01:46:38.651952700Z",
     "start_time": "2023-11-09T01:46:38.631664500Z"
    }
   },
   "id": "17440b50a7ccdcd4"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "[35,\n 10205,\n 358,\n 68,\n 1556,\n 6557,\n 4312,\n 75,\n 39588,\n 30,\n 1001,\n 2207,\n 84,\n 298,\n 430,\n 551,\n 47352,\n 260,\n 11,\n 38496,\n 11,\n 257,\n 513,\n 18912,\n 324,\n 8847,\n 390,\n 8591,\n 42433,\n 10033]"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(data[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T01:47:55.463540800Z",
     "start_time": "2023-11-09T01:47:55.442906300Z"
    }
   },
   "id": "a9d5547b3329dfb1"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextDataset.__init__() got an unexpected keyword argument 'text_column'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m tokenized_data \u001B[38;5;241m=\u001B[39m tokenizer(data, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Crear un conjunto de datos\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mTextDataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfile_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdummy_file.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Utilizamos un nombre de archivo ficticio\u001B[39;49;00m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext_column\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Nombre de la columna que contiene los textos\u001B[39;49;00m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_frame\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenized_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdummy_file.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Esto es necesario para evitar el error\u001B[39;49;00m\n\u001B[0;32m     11\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Preparar el DataCollator\u001B[39;00m\n\u001B[0;32m     14\u001B[0m data_collator \u001B[38;5;241m=\u001B[39m DataCollatorForLanguageModeling(\n\u001B[0;32m     15\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mtokenizer,\n\u001B[0;32m     16\u001B[0m     mlm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     17\u001B[0m )\n",
      "\u001B[1;31mTypeError\u001B[0m: TextDataset.__init__() got an unexpected keyword argument 'text_column'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenizar los datos\n",
    "tokenized_data = tokenizer(data, truncation=True, padding=True)\n",
    "\n",
    "# Crear un conjunto de datos\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"dummy_file.txt\",  # Utilizamos un nombre de archivo ficticio\n",
    "    text_column=\"text\",  # Nombre de la columna que contiene los textos\n",
    "    data_frame=tokenized_data,\n",
    "    data_files={\"train\": [\"dummy_file.txt\"]},  # Esto es necesario para evitar el error\n",
    ")\n",
    "\n",
    "# Preparar el DataCollator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Definir"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T01:05:45.121478Z",
     "start_time": "2023-11-09T01:05:45.087371700Z"
    }
   },
   "id": "a2d8bf6632e0c5b"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TFGPT2LMHeadModel' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 12\u001B[0m\n\u001B[0;32m      2\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[0;32m      3\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./gpt2-fine-tuned\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      4\u001B[0m     overwrite_output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      8\u001B[0m     save_total_limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m      9\u001B[0m )\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Crear el objeto Trainer y realizar el fine-tuning\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_collator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_collator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Guardar el modelo fine-tuned\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Inteligencia Artificial\\venv\\Lib\\site-packages\\transformers\\trainer.py:481\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001B[0m\n\u001B[0;32m    476\u001B[0m \u001B[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001B[39;00m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mplace_model_on_device\n\u001B[0;32m    479\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquantization_method\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m QuantizationMethod\u001B[38;5;241m.\u001B[39mBITS_AND_BYTES\n\u001B[0;32m    480\u001B[0m ):\n\u001B[1;32m--> 481\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_move_model_to_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001B[39;00m\n\u001B[0;32m    484\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_model_parallel:\n",
      "File \u001B[1;32mE:\\Inteligencia Artificial\\venv\\Lib\\site-packages\\transformers\\trainer.py:716\u001B[0m, in \u001B[0;36mTrainer._move_model_to_device\u001B[1;34m(self, model, device)\u001B[0m\n\u001B[0;32m    715\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_move_model_to_device\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, device):\n\u001B[1;32m--> 716\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m(device)\n\u001B[0;32m    717\u001B[0m     \u001B[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001B[39;00m\n\u001B[0;32m    718\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mparallel_mode \u001B[38;5;241m==\u001B[39m ParallelMode\u001B[38;5;241m.\u001B[39mTPU \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtie_weights\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'TFGPT2LMHeadModel' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-fine-tuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataloader,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Guardar el modelo fine-tuned\n",
    "trainer.save_model()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T23:04:12.913823900Z",
     "start_time": "2023-11-09T23:04:12.806519100Z"
    }
   },
   "id": "f58e14c2d484b061"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionalidad de la entrada: torch.Size([1, 17])\n",
      "Dimensionalidad de los logits de inicio: torch.Size([1, 17])\n",
      "Dimensionalidad de los logits de fin: torch.Size([1, 17])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Cargar el modelo preentrenado y el tokenizador\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Ejemplo de entrada: pregunta y contexto\n",
    "question = \"What is the capital of France?\"\n",
    "context = \"Paris is the capital of France.\"\n",
    "\n",
    "# Tokenizar la entrada\n",
    "inputs = tokenizer(question, context, return_tensors='pt')\n",
    "\n",
    "# Pasar la entrada a través del modelo\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Obtener los logits asociados con cada token en la secuencia tokenizada\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "print(\"Dimensionalidad de la entrada:\", inputs['input_ids'].shape)\n",
    "print(\"Dimensionalidad de los logits de inicio:\", start_logits.shape)\n",
    "print(\"Dimensionalidad de los logits de fin:\", end_logits.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T10:44:56.906902100Z",
     "start_time": "2023-11-16T10:44:46.274213700Z"
    }
   },
   "id": "e873618de844c666"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.1115, -0.2291,  0.2048,  0.5086, -0.1032,  0.3235,  0.2025, -0.1516,\n          0.1464, -0.1851,  0.2534,  0.5938, -0.0069,  0.2593,  0.2905,  0.2370,\n          0.2380]], grad_fn=<CloneBackward0>)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T10:45:24.027703200Z",
     "start_time": "2023-11-16T10:45:23.995996400Z"
    }
   },
   "id": "91541b59c74bbaf6"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.1782,  0.2064, -0.0011, -0.2340, -0.3734, -0.0957, -0.4119,  0.1857,\n         -0.1770, -0.4938, -0.0999, -0.1218, -0.3205,  0.0053, -0.2130, -0.1041,\n         -0.0948]], grad_fn=<CloneBackward0>)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T10:45:24.604366600Z",
     "start_time": "2023-11-16T10:45:24.582664500Z"
    }
   },
   "id": "8122829c363d3ca8"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[ 101, 2054, 2003, 1996, 3007, 1997, 2605, 1029,  102, 3000, 2003, 1996,\n         3007, 1997, 2605, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T10:45:26.512485Z",
     "start_time": "2023-11-16T10:45:26.491795400Z"
    }
   },
   "id": "50eb331bbce3b642"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.1115, -0.2291,  0.2048,  0.5086, -0.1032,  0.3235,  0.2025, -0.1516,\n          0.1464, -0.1851,  0.2534,  0.5938, -0.0069,  0.2593,  0.2905,  0.2370,\n          0.2380]], grad_fn=<CloneBackward0>)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#end_position = torch.argmax(end_logits).item()\n",
    "outputs.start_logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T10:52:12.156474800Z",
     "start_time": "2023-11-16T10:52:12.117447700Z"
    }
   },
   "id": "b0a14d3738723ebb"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.1782,  0.2064, -0.0011, -0.2340, -0.3734, -0.0957, -0.4119,  0.1857,\n         -0.1770, -0.4938, -0.0999, -0.1218, -0.3205,  0.0053, -0.2130, -0.1041,\n         -0.0948]], grad_fn=<CloneBackward0>)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.end_logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T11:23:19.728785900Z",
     "start_time": "2023-11-16T11:23:19.690687Z"
    }
   },
   "id": "fe2cec6b9677a31e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4493b069f5c0c8f5"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "11"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(outputs.start_logits).item()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T11:23:47.469964500Z",
     "start_time": "2023-11-16T11:23:47.462335100Z"
    }
   },
   "id": "3dc7a6f58a2c20ec"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.1218, -0.3205,  0.0053, -0.2130, -0.1041, -0.0948],\n       grad_fn=<SliceBackward0>)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_slice = outputs.end_logits[0,11:]\n",
    "outputs_slice"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T10:57:34.747789300Z",
     "start_time": "2023-11-16T10:57:34.730650300Z"
    }
   },
   "id": "a1270c2c099bc96"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(2)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(outputs_slice)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T10:57:36.065387300Z",
     "start_time": "2023-11-16T10:57:36.042065700Z"
    }
   },
   "id": "a525a9671aa25f77"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 101, 2054, 2003, 1996, 3007, 1997, 2605, 1029,  102, 3000, 2003, 1996,\n        3007, 1997, 2605, 1012,  102])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'][0][:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T11:12:31.688572200Z",
     "start_time": "2023-11-16T11:12:31.651035100Z"
    }
   },
   "id": "44f681a8ad4ce7c0"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta Final: the capital of\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar la posición de inicio con el valor más alto\n",
    "start_position = torch.argmax(outputs.start_logits).item()\n",
    "\n",
    "# Seleccionar la posición de fin después de la posición de inicio\n",
    "end_logits_slice = outputs.end_logits[0,start_position:]\n",
    "end_position = torch.argmax(end_logits_slice).item() + start_position\n",
    "\n",
    "# Des-Tokenización para obtener la respuesta final\n",
    "answer = tokenizer.decode(inputs['input_ids'][0, start_position:end_position + 1].tolist())\n",
    "\n",
    "print(\"Respuesta Final:\", answer)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T11:11:17.630701200Z",
     "start_time": "2023-11-16T11:11:17.590814600Z"
    }
   },
   "id": "87c118eb87de7111"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "answer = tokenizer.decode(inputs['input_ids'][0, start_position:end_position+1].tolist(), skip_special_tokens=True)\n",
    "\n",
    "print(\"Respuesta Final:\", answer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T02:09:07.745169400Z",
     "start_time": "2023-11-16T02:09:07.742179700Z"
    }
   },
   "id": "a247eb34ed833932"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "15"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_position.item()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T02:03:21.468000700Z",
     "start_time": "2023-11-16T02:03:21.442107600Z"
    }
   },
   "id": "46086e3e9a7e665f"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(15)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_position"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T02:03:22.010388500Z",
     "start_time": "2023-11-16T02:03:21.980094600Z"
    }
   },
   "id": "9eeaf82800e24663"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[ 101, 2054, 2003, 1996, 3007, 1997, 2605, 1029,  102, 3000, 2003, 1996,\n         3007, 1997, 2605, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T02:04:12.265869800Z",
     "start_time": "2023-11-16T02:04:12.220276700Z"
    }
   },
   "id": "67c124ee2d9c6c29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5d326613aae3bfb8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
