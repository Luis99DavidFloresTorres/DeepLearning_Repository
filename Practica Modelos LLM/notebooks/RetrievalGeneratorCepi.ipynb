{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/812 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2a70d4d8581469ab7e7d4e3a6bdf8d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Git Hub Inteligencia Artificial\\Practica Modelos LLM\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Usuario\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/3.06G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d228e6d71f34430b8c1fcbd649d3134"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/39.9k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a6b8d45122f444da920cd6fe96c8d79"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d165af76ae914c85af73d75a64811347"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "837d2db1b9d9437bbef2a13046e09868"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/4.23k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9ccb15bcb3d48b2a544a7fdbec48d76"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1024])\n",
      "tensor([[-0.0053,  0.0020, -0.0006,  ...,  0.0094, -0.0009,  0.0070],\n",
      "        [-0.0003, -0.0071,  0.0076,  ...,  0.0055,  0.0022, -0.0083]])\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers sentencepiece -q\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.m2m_100.modeling_m2m_100 import M2M100Encoder\n",
    "\n",
    "model_name = \"cointegrated/SONAR_200_text_encoder\"\n",
    "encoder = M2M100Encoder.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def encode_mean_pool(texts, tokenizer, encoder, lang='eng_Latn', norm=False):\n",
    "    tokenizer.src_lang = lang\n",
    "    with torch.inference_mode():\n",
    "        batch = tokenizer(texts, return_tensors='pt', padding=True)\n",
    "        seq_embs = encoder(**batch).last_hidden_state\n",
    "        mask = batch.attention_mask\n",
    "        mean_emb = (seq_embs * mask.unsqueeze(-1)).sum(1) / mask.unsqueeze(-1).sum(1)\n",
    "        if norm:\n",
    "            mean_emb = torch.nn.functional.normalize(mean_emb)\n",
    "    return mean_emb\n",
    "\n",
    "sentences = ['My name is SONAR.', 'I can embed the sentences into vectorial space.']\n",
    "embs = encode_mean_pool(sentences, tokenizer, encoder, lang=\"eng_Latn\")\n",
    "print(embs.shape)  \n",
    "# torch.Size([2, 1024])\n",
    "print(embs)\n",
    "# tensor([[-0.0053,  0.0020, -0.0006,  ...,  0.0094, -0.0009,  0.0070],\n",
    "#         [-0.0003, -0.0071,  0.0076,  ...,  0.0055,  0.0022, -0.0083]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T15:50:37.231884100Z",
     "start_time": "2024-01-10T15:37:18.022889600Z"
    }
   },
   "id": "9ff42d7ad43667c1"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1024])\n",
      "tensor([[ 3.2370e-04,  2.3489e-03,  9.9993e-05,  ..., -8.4636e-03,\n",
      "          7.5012e-03,  3.8039e-03],\n",
      "        [ 3.3168e-03,  2.4580e-03, -2.8700e-03,  ..., -1.0706e-02,\n",
      "          3.2984e-03, -6.1285e-04]])\n"
     ]
    }
   ],
   "source": [
    "sentences = ['Cat is a animal', 'I have a cat i have a animal']\n",
    "embs = encode_mean_pool(sentences, tokenizer, encoder, lang=\"eng_Latn\")\n",
    "print(embs.shape)  \n",
    "# torch.Size([2, 1024])\n",
    "print(embs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T16:21:58.662221900Z",
     "start_time": "2024-01-10T16:21:58.082831700Z"
    }
   },
   "id": "ca6063e9862b33ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pip"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "730f1d098ecc8929"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "271894a4593a444d9f4a9011ce12cead"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Git Hub Inteligencia Artificial\\Practica Modelos LLM\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Usuario\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/489 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8320a6f9b84741e0a977f59e1fc7df56"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/242k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0fa0b169370c4eeeb3e7f9a79f344ba7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cc8fed57fa94dc18ba429a0474d6a6c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "809c5f9290c8451c8b9edcaa9dd4002a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sadakmed/dpr-passage_encoder-spanish\")\n",
    "model = BertModel.from_pretrained(\"sadakmed/dpr-passage_encoder-spanish\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T14:41:22.198336600Z",
     "start_time": "2024-01-11T14:40:03.017591100Z"
    }
   },
   "id": "fd6747480a22d32a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "query = \"Disculpe buenas tardes el  Diplomado en Banca a cuanto esta?\"\n",
    "context = '''programa : diplomado en banca y transformación digital \n",
    "certificado academico : diplomado en banca y transformación digital\n",
    "unidad de ejecucion : Centro de Estudios de Posgrado e Investigación\n",
    "Facultad de Contaduría Pública y Ciencias Financieras\n",
    "sede :  SUCRE: \n",
    "CEPI: Calle Aniceto Arce No. 46, zona central.\n",
    "Telf. y Fax: (591) 4 6440887\n",
    "Facultad de Contaduría Pública y Ciencias Financieras: Calle Grau Nº 149  .\n",
    "TLF: (591) 4 6434025 - (591) 4 6452320\n",
    "costo total : Bs \t3.600, 00\n",
    "modalidad de ejecucion : virtual\n",
    "'''\n",
    "token = tokenizer.encode(query,context,return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T15:06:19.648090100Z",
     "start_time": "2024-01-11T15:06:19.631178900Z"
    }
   },
   "id": "eae10780345070e9"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model_output = model(token)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T15:26:42.794875Z",
     "start_time": "2024-01-11T15:26:42.516943400Z"
    }
   },
   "id": "5ee284f0a82e6088"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 768])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.pooler_output.size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T15:26:56.524120400Z",
     "start_time": "2024-01-11T15:26:56.505864800Z"
    }
   },
   "id": "47f9541a6bb48e47"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 768])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.pooler_output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T15:38:05.695973Z",
     "start_time": "2024-01-11T15:38:05.680916900Z"
    }
   },
   "id": "853d2cda4472f20d"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "odict_keys(['last_hidden_state', 'pooler_output'])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T15:42:52.054599200Z",
     "start_time": "2024-01-11T15:42:52.023281300Z"
    }
   },
   "id": "682028dd3e8eaca"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DPRQuestionEncoder were not initialized from the model checkpoint at IIC/dpr-spanish-question_encoder-allqa-base and are newly initialized: ['bert_model.encoder.layer.5.output.LayerNorm.weight', 'bert_model.encoder.layer.11.output.LayerNorm.bias', 'bert_model.encoder.layer.5.attention.output.dense.weight', 'bert_model.encoder.layer.7.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.6.attention.self.query.weight', 'bert_model.encoder.layer.7.intermediate.dense.bias', 'bert_model.encoder.layer.4.output.LayerNorm.weight', 'bert_model.encoder.layer.10.attention.self.query.weight', 'bert_model.encoder.layer.9.intermediate.dense.bias', 'bert_model.encoder.layer.9.attention.self.key.bias', 'bert_model.encoder.layer.0.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.11.attention.self.query.bias', 'bert_model.encoder.layer.11.output.dense.weight', 'bert_model.encoder.layer.6.attention.output.dense.weight', 'bert_model.encoder.layer.9.attention.self.value.bias', 'bert_model.encoder.layer.7.output.dense.weight', 'bert_model.encoder.layer.11.attention.self.key.weight', 'bert_model.encoder.layer.6.intermediate.dense.weight', 'bert_model.encoder.layer.9.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.8.attention.output.dense.weight', 'bert_model.encoder.layer.5.attention.self.query.bias', 'bert_model.encoder.layer.6.attention.self.value.weight', 'bert_model.encoder.layer.6.attention.self.key.weight', 'bert_model.encoder.layer.1.output.dense.weight', 'bert_model.encoder.layer.0.attention.self.key.weight', 'bert_model.encoder.layer.2.output.LayerNorm.weight', 'bert_model.encoder.layer.10.output.LayerNorm.weight', 'bert_model.encoder.layer.2.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.1.attention.output.dense.bias', 'bert_model.encoder.layer.0.output.LayerNorm.bias', 'bert_model.embeddings.word_embeddings.weight', 'bert_model.encoder.layer.7.output.LayerNorm.weight', 'bert_model.encoder.layer.1.attention.self.key.weight', 'bert_model.encoder.layer.9.attention.self.query.bias', 'bert_model.encoder.layer.1.attention.output.dense.weight', 'bert_model.encoder.layer.5.intermediate.dense.weight', 'bert_model.encoder.layer.11.attention.output.dense.bias', 'bert_model.embeddings.position_embeddings.weight', 'bert_model.encoder.layer.0.output.dense.weight', 'bert_model.encoder.layer.7.attention.self.query.weight', 'bert_model.encoder.layer.1.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.2.attention.output.dense.bias', 'bert_model.encoder.layer.9.attention.self.query.weight', 'bert_model.encoder.layer.11.attention.self.key.bias', 'bert_model.encoder.layer.4.output.dense.bias', 'bert_model.encoder.layer.7.intermediate.dense.weight', 'bert_model.encoder.layer.4.attention.self.value.weight', 'bert_model.encoder.layer.11.attention.self.query.weight', 'bert_model.encoder.layer.7.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.4.attention.self.query.bias', 'bert_model.encoder.layer.10.attention.self.key.weight', 'bert_model.encoder.layer.3.attention.output.dense.weight', 'bert_model.encoder.layer.7.attention.self.key.bias', 'bert_model.encoder.layer.5.attention.self.value.weight', 'bert_model.encoder.layer.8.attention.self.key.bias', 'bert_model.encoder.layer.3.attention.output.dense.bias', 'bert_model.encoder.layer.5.attention.self.value.bias', 'bert_model.encoder.layer.5.attention.output.dense.bias', 'bert_model.encoder.layer.1.output.LayerNorm.bias', 'bert_model.encoder.layer.7.attention.self.value.weight', 'bert_model.encoder.layer.5.output.LayerNorm.bias', 'bert_model.encoder.layer.9.attention.output.dense.bias', 'bert_model.encoder.layer.5.attention.self.key.weight', 'bert_model.encoder.layer.4.attention.self.value.bias', 'bert_model.encoder.layer.2.attention.self.key.bias', 'bert_model.encoder.layer.0.attention.self.value.bias', 'bert_model.encoder.layer.4.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.3.attention.self.value.bias', 'bert_model.encoder.layer.9.output.LayerNorm.weight', 'bert_model.encoder.layer.8.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.2.output.dense.bias', 'bert_model.encoder.layer.3.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.9.output.dense.weight', 'bert_model.encoder.layer.9.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.0.attention.self.key.bias', 'bert_model.encoder.layer.1.attention.self.query.weight', 'bert_model.encoder.layer.1.attention.self.key.bias', 'bert_model.encoder.layer.4.attention.self.query.weight', 'bert_model.encoder.layer.11.output.dense.bias', 'bert_model.encoder.layer.9.attention.self.value.weight', 'bert_model.encoder.layer.10.attention.self.value.weight', 'bert_model.encoder.layer.0.intermediate.dense.bias', 'bert_model.encoder.layer.9.attention.self.key.weight', 'bert_model.encoder.layer.0.attention.output.dense.weight', 'bert_model.encoder.layer.4.intermediate.dense.weight', 'bert_model.encoder.layer.4.attention.self.key.bias', 'bert_model.encoder.layer.10.attention.self.query.bias', 'bert_model.encoder.layer.10.intermediate.dense.weight', 'bert_model.encoder.layer.8.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.0.output.dense.bias', 'bert_model.encoder.layer.7.attention.output.dense.weight', 'bert_model.encoder.layer.6.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.3.attention.self.value.weight', 'bert_model.encoder.layer.7.output.dense.bias', 'bert_model.encoder.layer.2.intermediate.dense.bias', 'bert_model.encoder.layer.8.output.dense.bias', 'bert_model.encoder.layer.3.attention.self.query.weight', 'bert_model.encoder.layer.5.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.0.attention.self.value.weight', 'bert_model.encoder.layer.6.attention.self.query.bias', 'bert_model.encoder.layer.2.output.dense.weight', 'bert_model.encoder.layer.0.intermediate.dense.weight', 'bert_model.encoder.layer.8.attention.self.value.weight', 'bert_model.encoder.layer.3.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.4.attention.output.dense.weight', 'bert_model.encoder.layer.8.attention.self.value.bias', 'bert_model.encoder.layer.8.attention.self.query.weight', 'bert_model.encoder.layer.11.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.3.attention.self.key.weight', 'bert_model.encoder.layer.10.attention.self.key.bias', 'bert_model.encoder.layer.9.intermediate.dense.weight', 'bert_model.encoder.layer.1.attention.self.value.bias', 'bert_model.encoder.layer.3.intermediate.dense.bias', 'bert_model.encoder.layer.6.attention.self.key.bias', 'bert_model.encoder.layer.5.attention.self.key.bias', 'bert_model.encoder.layer.1.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.1.output.dense.bias', 'bert_model.encoder.layer.3.output.LayerNorm.bias', 'bert_model.encoder.layer.11.output.LayerNorm.weight', 'bert_model.encoder.layer.6.attention.self.value.bias', 'bert_model.encoder.layer.10.output.LayerNorm.bias', 'bert_model.encoder.layer.6.output.dense.bias', 'bert_model.encoder.layer.10.attention.self.value.bias', 'bert_model.encoder.layer.2.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.4.output.dense.weight', 'bert_model.encoder.layer.1.output.LayerNorm.weight', 'bert_model.encoder.layer.2.intermediate.dense.weight', 'bert_model.encoder.layer.5.attention.self.query.weight', 'bert_model.encoder.layer.7.attention.self.query.bias', 'bert_model.encoder.layer.5.output.dense.bias', 'bert_model.encoder.layer.3.output.dense.weight', 'bert_model.encoder.layer.0.attention.self.query.bias', 'bert_model.encoder.layer.5.output.dense.weight', 'bert_model.encoder.layer.8.attention.self.query.bias', 'bert_model.encoder.layer.0.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.8.output.LayerNorm.bias', 'bert_model.encoder.layer.10.intermediate.dense.bias', 'bert_model.encoder.layer.10.output.dense.bias', 'bert_model.encoder.layer.11.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.11.attention.self.value.weight', 'bert_model.encoder.layer.9.output.dense.bias', 'bert_model.encoder.layer.11.attention.self.value.bias', 'bert_model.encoder.layer.1.attention.self.value.weight', 'bert_model.encoder.layer.3.output.dense.bias', 'bert_model.encoder.layer.2.attention.self.value.bias', 'bert_model.encoder.layer.7.attention.self.value.bias', 'bert_model.encoder.layer.4.intermediate.dense.bias', 'bert_model.encoder.layer.8.intermediate.dense.bias', 'bert_model.encoder.layer.3.output.LayerNorm.weight', 'bert_model.encoder.layer.9.attention.output.dense.weight', 'bert_model.encoder.layer.10.attention.output.dense.weight', 'bert_model.encoder.layer.8.attention.self.key.weight', 'bert_model.encoder.layer.6.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.4.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.10.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.3.attention.self.key.bias', 'bert_model.encoder.layer.10.attention.output.dense.bias', 'bert_model.encoder.layer.2.attention.self.key.weight', 'bert_model.encoder.layer.6.output.dense.weight', 'bert_model.encoder.layer.5.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.1.intermediate.dense.weight', 'bert_model.encoder.layer.7.attention.output.dense.bias', 'bert_model.encoder.layer.3.attention.self.query.bias', 'bert_model.embeddings.LayerNorm.weight', 'bert_model.encoder.layer.4.attention.output.dense.bias', 'bert_model.embeddings.LayerNorm.bias', 'bert_model.encoder.layer.9.output.LayerNorm.bias', 'bert_model.encoder.layer.11.attention.output.dense.weight', 'bert_model.encoder.layer.6.intermediate.dense.bias', 'bert_model.encoder.layer.1.intermediate.dense.bias', 'bert_model.embeddings.token_type_embeddings.weight', 'bert_model.encoder.layer.7.output.LayerNorm.bias', 'bert_model.encoder.layer.0.attention.self.query.weight', 'bert_model.encoder.layer.4.output.LayerNorm.bias', 'bert_model.encoder.layer.0.output.LayerNorm.weight', 'bert_model.encoder.layer.8.attention.output.dense.bias', 'bert_model.encoder.layer.2.attention.self.query.weight', 'bert_model.encoder.layer.3.intermediate.dense.weight', 'bert_model.encoder.layer.8.intermediate.dense.weight', 'bert_model.encoder.layer.8.output.dense.weight', 'bert_model.encoder.layer.11.intermediate.dense.bias', 'bert_model.encoder.layer.2.attention.self.query.bias', 'bert_model.encoder.layer.1.attention.self.query.bias', 'bert_model.encoder.layer.4.attention.self.key.weight', 'bert_model.encoder.layer.6.output.LayerNorm.weight', 'bert_model.encoder.layer.6.attention.output.dense.bias', 'bert_model.encoder.layer.5.intermediate.dense.bias', 'bert_model.encoder.layer.8.output.LayerNorm.weight', 'bert_model.encoder.layer.2.attention.output.dense.weight', 'bert_model.encoder.layer.6.output.LayerNorm.bias', 'bert_model.encoder.layer.11.intermediate.dense.weight', 'bert_model.encoder.layer.0.attention.output.dense.bias', 'bert_model.encoder.layer.10.output.dense.weight', 'bert_model.encoder.layer.2.output.LayerNorm.bias', 'bert_model.encoder.layer.2.attention.self.value.weight', 'bert_model.encoder.layer.10.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.7.attention.self.key.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "\n",
    "model_str = \"IIC/dpr-spanish-question_encoder-allqa-base\"\n",
    "tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(model_str)\n",
    "model = DPRQuestionEncoder.from_pretrained(model_str)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T01:51:51.421265800Z",
     "start_time": "2024-01-12T01:51:49.062490100Z"
    }
   },
   "id": "8ff4d6d927e8bb1f"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def model_query_output(word):\n",
    "    input_ids = tokenizer(word, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    query_embeddings = model(input_ids).pooler_output\n",
    "    return query_embeddings.detach().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T02:21:25.603370700Z",
     "start_time": "2024-01-12T02:21:25.585744300Z"
    }
   },
   "id": "a1e2b795fe4554"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-6.72719419e-01, -4.45536263e-02,  8.14662218e-01,\n        -1.79185390e-01, -1.64071977e-01, -2.39909863e+00,\n        -5.92900991e-01, -9.53954220e-01,  2.48369742e-02,\n        -1.29709363e+00,  1.90237403e+00,  5.54110467e-01,\n        -9.99662220e-01, -3.71938199e-01,  5.75284474e-02,\n        -4.67096835e-01,  7.82199681e-01, -4.91334617e-01,\n        -9.97006357e-01, -1.29229021e+00, -1.62421358e+00,\n         1.14575577e+00, -6.72953069e-01, -1.06598401e+00,\n         1.95739603e+00, -3.22984546e-01, -6.57687187e-01,\n         6.29309788e-02, -1.06082189e+00,  2.07091093e-01,\n         9.94658828e-01, -2.13912189e-01,  1.62228525e-01,\n        -7.77383029e-01,  3.53976071e-01,  1.27271414e+00,\n         3.21519494e-01,  3.72279793e-01, -3.14089745e-01,\n         1.51762173e-01, -4.12794828e-01, -1.12807512e+00,\n        -1.59266818e+00,  1.94258541e-01, -2.37690777e-01,\n         1.33744824e+00,  3.23186576e-01,  3.46380174e-01,\n         5.83737791e-01,  3.65897059e-01,  9.61071968e-01,\n         5.99611938e-01,  1.25901878e+00,  6.60298020e-02,\n         6.20567977e-01, -9.96316373e-01,  3.06727082e-01,\n         1.60491228e+00,  5.07058442e-01,  9.70134974e-01,\n        -1.01817822e+00, -3.98948073e-01,  1.19798625e+00,\n        -1.26680779e+00,  5.13911188e-01, -3.84798855e-01,\n        -1.57790887e+00,  8.02788734e-01,  1.18538320e+00,\n        -1.81084469e-01,  4.49459881e-01,  4.79096800e-01,\n         4.35298324e-01,  1.56303024e+00, -4.38979298e-01,\n         4.73043978e-01, -6.10424757e-01,  2.03610621e-02,\n        -1.46201849e+00,  4.27847922e-01,  5.62780142e-01,\n         1.12675750e+00, -5.31000912e-01, -6.41881585e-01,\n         7.89452866e-02, -2.02604204e-01,  6.09069467e-01,\n        -4.71660435e-01,  1.58010912e+00, -1.19890165e+00,\n        -1.05114400e+00, -6.18564188e-01,  3.67442876e-01,\n         2.70767063e-01, -1.67083776e+00,  1.40643442e+00,\n        -1.97990382e+00,  6.71006739e-01, -8.60641718e-01,\n        -6.90485775e-01,  1.15905035e+00, -3.29450443e-02,\n        -2.76093841e-01,  6.28875852e-01,  1.31392205e+00,\n        -4.44683641e-01,  1.56584895e+00, -2.54290313e-01,\n         5.23621321e-01,  1.53627291e-01, -1.19417870e+00,\n        -1.14423192e+00,  9.03885067e-01, -1.76069736e+00,\n         3.86674106e-01,  1.15264046e+00, -8.74458671e-01,\n        -4.14981872e-01,  4.98299599e-01,  1.98047876e-01,\n        -4.63071555e-01,  1.86807680e+00,  2.42126417e+00,\n         1.04637170e+00,  6.78261578e-01,  1.01176918e+00,\n         1.16842818e+00, -2.86565542e-01,  4.62646127e-01,\n         8.34259570e-01, -4.18876857e-01,  6.54838741e-01,\n         1.76083788e-01,  1.31552958e+00,  8.86571109e-01,\n         3.79843861e-02, -1.24036217e+00,  7.15469792e-02,\n         8.23623955e-01, -1.47594237e+00,  6.51076972e-01,\n        -5.37038632e-02,  9.76734817e-01,  7.13697672e-01,\n        -1.36136508e+00, -7.35037446e-01, -5.08433044e-01,\n        -4.83954996e-01,  8.08285177e-01, -5.39938033e-01,\n         1.05529714e+00, -1.26249051e+00, -1.07373106e+00,\n        -6.76696539e-01, -5.86546063e-01, -3.47433537e-01,\n         1.45185798e-01, -1.50635493e+00,  7.13348389e-02,\n         4.20935810e-01,  3.42188716e-01, -8.90594006e-01,\n        -1.00498283e+00,  1.68140662e+00, -2.12583280e+00,\n         8.72731745e-01,  1.09070981e+00,  5.89485466e-01,\n        -1.35953963e+00, -9.10770535e-01,  1.12290132e+00,\n        -7.57019341e-01, -2.57548881e+00,  1.81869268e-02,\n         6.43041909e-01, -1.22982657e+00, -9.53746773e-03,\n        -1.01248264e+00,  1.00508265e-01,  1.18844068e+00,\n        -1.12044084e+00, -8.53696883e-01, -2.54734635e-01,\n         1.20114410e+00,  1.57603145e-01,  1.82811117e+00,\n         4.51483399e-01, -5.24827302e-01,  3.51822555e-01,\n        -3.17889929e-01,  2.02783966e+00,  1.84219110e+00,\n        -1.53984070e+00,  3.70357931e-01,  7.87242174e-01,\n         1.46464074e+00,  1.23716414e-01, -1.41741610e+00,\n         9.26695049e-01, -7.35611677e-01, -3.77986163e-01,\n        -3.30036730e-01,  4.39091623e-01,  2.68163919e-01,\n        -8.15642476e-01, -1.12376571e-01,  1.13271868e+00,\n         1.43612182e+00,  5.96836470e-02,  7.90565848e-01,\n         1.30059576e+00, -4.76067156e-01, -1.49773371e+00,\n        -8.91000628e-02, -2.89258242e-01, -1.29352045e+00,\n        -2.37856007e+00, -1.35936165e+00,  9.46375489e-01,\n         1.06395423e+00,  1.17492211e+00, -5.92693746e-01,\n        -6.15234971e-01, -7.44926631e-01, -9.03209567e-01,\n        -2.08545923e+00, -7.71988571e-01,  4.71783429e-01,\n        -3.89788002e-01,  6.70994699e-01, -5.60847282e-01,\n         2.00136498e-01,  1.00150682e-01,  7.79233277e-01,\n         5.62513769e-01, -1.79696262e+00,  2.48743773e-01,\n        -1.24463093e+00, -7.88408875e-01,  1.04218328e+00,\n        -3.32899839e-01, -1.45225561e+00, -3.92713457e-01,\n         6.09585404e-01, -1.09498608e+00, -2.20033437e-01,\n         1.47232503e-01,  3.87109488e-01, -4.73800302e-01,\n        -4.32621270e-01,  5.62849939e-01,  1.45258999e+00,\n         6.32382035e-01, -1.00783968e+00, -1.07641029e+00,\n         8.24759379e-02,  2.52234489e-01,  6.58205673e-02,\n         1.39385855e+00,  5.21801412e-01,  6.49418175e-01,\n         9.94584978e-01,  2.90052593e-01,  1.05543411e+00,\n        -1.05423200e+00, -6.71097934e-01, -6.54899657e-01,\n        -1.36171067e+00, -1.33863616e+00,  2.15569884e-01,\n        -2.95727223e-01, -3.77660483e-01,  1.41057765e+00,\n        -1.95771039e-01,  1.13823605e+00,  9.51323688e-01,\n         1.36396497e-01, -3.87926966e-01,  5.86309731e-01,\n         1.50108740e-01,  2.09968612e-02, -1.34124863e+00,\n         3.61663610e-01, -3.73332322e-01, -7.92253196e-01,\n        -2.27734971e+00,  1.16210803e-01,  6.89068973e-01,\n        -8.78387019e-02,  1.10251606e+00,  7.72775650e-01,\n         8.85510921e-01,  7.07911313e-01, -4.85173345e-01,\n         6.00765646e-02, -1.69499032e-02,  9.77263033e-01,\n         1.13621402e+00, -3.07118118e-01,  3.45009118e-01,\n         9.72638801e-02, -6.95957661e-01,  1.01591873e+00,\n         1.89158785e+00, -1.04958653e+00,  5.02084315e-01,\n        -9.65763748e-01,  2.03490257e+00, -1.39562666e+00,\n         1.74814773e+00,  1.12496650e+00, -2.69005239e-01,\n         4.96529371e-01,  3.36547703e-01, -3.54378891e+00,\n         1.77145493e+00, -4.07710969e-01,  1.27910280e+00,\n        -1.05559719e+00, -1.64357269e+00,  9.87064123e-01,\n         1.13517034e+00,  8.10174882e-01,  9.39033210e-01,\n         5.32441258e-01, -8.54648948e-02, -1.35525966e+00,\n        -3.16509008e-01,  8.62909853e-02,  7.88096011e-01,\n        -1.39513958e+00,  4.94090587e-01, -1.01394546e+00,\n        -8.30124974e-01, -5.12043498e-02,  9.70542356e-02,\n        -2.34561548e-01, -1.57325780e+00,  2.34746790e+00,\n        -2.97827065e-01, -1.11015046e+00, -7.48811901e-01,\n         1.30398750e+00, -1.63406360e+00,  8.51480186e-01,\n        -1.00706053e+00, -1.05228829e+00, -5.40932901e-02,\n        -2.50152320e-01, -2.32128215e+00,  7.71061897e-01,\n        -4.29444075e-01,  1.11111903e+00,  1.90681636e+00,\n        -6.87594175e-01, -1.72712639e-01, -2.65510499e-01,\n        -2.33668542e+00, -1.31181073e+00,  7.17041075e-01,\n         1.21726215e+00, -6.23689234e-01,  1.63421488e+00,\n         1.50477529e+00,  1.81346393e+00,  1.60137677e+00,\n         1.26670003e+00,  8.87056291e-02,  8.86792421e-01,\n        -4.36546683e-01,  2.60942757e-01,  5.13229489e-01,\n         1.34055662e+00,  5.25669277e-01, -9.60644364e-01,\n        -2.02683091e-01, -8.70829761e-01,  4.41186726e-01,\n         2.63541031e+00,  1.71476507e+00, -1.31080169e-02,\n         1.22238255e+00,  2.02833343e+00,  4.40682560e-01,\n         1.67433667e+00, -1.22149420e+00,  6.77426875e-01,\n        -2.10492039e+00, -5.10720670e-01,  1.52739331e-01,\n        -7.28898287e-01,  9.88282621e-01,  5.19777179e-01,\n         8.68341625e-01, -1.18467236e+00, -1.03307627e-01,\n         9.62402284e-01,  7.52133191e-01, -3.30829360e-02,\n        -6.00370109e-01, -7.45947897e-01,  9.44586635e-01,\n         1.46778214e+00,  8.42570901e-01, -2.80354053e-01,\n        -6.56075060e-01, -1.49738342e-01, -6.02532774e-02,\n        -1.21263206e+00,  5.85147262e-01,  1.54092598e+00,\n        -5.01020551e-01,  8.24666619e-01, -1.56685293e+00,\n         2.24713850e+00, -1.12432921e+00,  1.20137370e+00,\n        -4.20906067e-01,  3.31113309e-01, -7.50935003e-02,\n        -1.72940180e-01,  9.11210954e-01,  2.73783449e-02,\n         1.27015448e+00, -1.71075389e-01,  1.70017228e-01,\n         1.40298939e+00, -5.17719746e-01, -6.49492264e-01,\n        -1.15392590e+00,  1.32806170e+00,  1.11175430e+00,\n        -8.91252041e-01,  9.09023464e-01,  1.54774711e-01,\n        -7.59823799e-01,  8.72941673e-01,  9.04949382e-03,\n         6.55733868e-02,  9.56707418e-01, -2.00415879e-01,\n         5.93489230e-01,  9.13199484e-01, -1.44198835e+00,\n        -1.64013410e+00, -3.31690192e-01, -1.00916994e+00,\n         1.09341490e+00,  1.05164684e-01, -4.54113156e-01,\n         1.24042177e+00,  1.83057100e-01,  5.33083558e-01,\n        -7.79259652e-02, -1.97958723e-01,  1.34086800e+00,\n        -3.44811082e-01, -1.51379004e-01, -9.90369737e-01,\n         8.94110143e-01, -1.18775666e+00,  2.34576076e-01,\n        -1.32159233e-01, -1.26782990e+00,  1.18654597e+00,\n         3.07737380e-01, -1.09206653e+00,  8.30074728e-01,\n         1.31272221e+00,  7.07445025e-01,  8.25148076e-02,\n         5.58904290e-01,  5.44001162e-01,  2.61432733e-02,\n         4.24584180e-01, -7.59641647e-01, -7.05287576e-01,\n         4.84273523e-01, -2.29853678e+00, -1.71238750e-01,\n         9.22020555e-01,  1.24011767e+00, -6.84881568e-01,\n        -1.64889395e+00, -4.43149686e-01, -1.03214972e-01,\n        -1.41130701e-01,  2.82718867e-01,  1.48334950e-01,\n        -7.63502359e-01, -4.33687538e-01,  3.08372885e-01,\n         2.02094004e-01,  1.34382546e-01, -1.49610126e+00,\n         1.60149872e-01, -1.00894362e-01,  5.22173941e-01,\n         7.57857084e-01, -6.54021949e-02,  3.48082006e-01,\n         1.55733299e+00,  5.60229480e-01,  2.25016728e-01,\n        -1.04978263e+00,  1.80026031e+00,  1.05994689e+00,\n         4.07165915e-01, -8.55438769e-01, -1.98183918e+00,\n        -3.77963632e-01,  1.78899384e+00, -7.48760104e-01,\n        -7.29046464e-01,  1.48299706e+00, -5.33426479e-02,\n         2.88889527e-01,  3.44298959e-01,  7.05489337e-01,\n        -9.70486701e-01,  5.01604974e-01, -2.14485094e-01,\n         4.02742267e-01,  5.42361997e-02, -4.71026212e-01,\n        -9.96953025e-02,  1.84458351e+00, -7.55839705e-01,\n        -1.32482529e+00,  1.18152253e-01, -1.82479215e+00,\n         1.53837800e+00, -5.12135625e-01, -1.75613895e-01,\n        -1.79944739e-01,  1.96840435e-01, -2.73407245e+00,\n         1.45682168e+00,  1.04091573e+00,  3.37186098e-01,\n        -8.28244746e-01, -5.10059536e-01, -9.39412653e-01,\n         5.96220553e-01, -4.83647376e-01,  5.52607119e-01,\n        -1.50560009e+00,  4.20314670e-01, -2.80882508e-01,\n         1.11321598e-01,  1.48791540e+00, -4.52163398e-01,\n        -1.76593852e+00,  6.72779620e-01,  1.29856634e+00,\n         7.08031282e-02,  4.57302600e-01, -1.77042568e+00,\n        -7.72568583e-01, -2.73022503e-01,  5.70023298e-01,\n        -4.50388283e-01, -1.70039403e+00,  2.27475554e-01,\n         1.28733385e+00, -1.22690761e+00,  6.73638344e-01,\n         8.14105988e-01, -8.77577662e-01, -1.64457083e-01,\n        -1.41326749e+00,  1.71440172e+00, -4.65197891e-01,\n        -2.27797508e+00,  9.31642771e-01,  1.46920398e-01,\n        -6.33389592e-01, -4.84696656e-01, -7.73277640e-01,\n         1.42155850e+00,  1.15078664e+00,  7.08098054e-01,\n         1.40498579e-01, -1.10551262e+00,  1.47988582e+00,\n        -7.84722686e-01,  1.43684244e+00, -7.83709824e-01,\n        -3.02382801e-02, -1.28057599e+00,  1.62186027e+00,\n        -1.45992899e+00,  9.47726548e-01, -6.25395894e-01,\n        -1.20055057e-01, -8.36890996e-01,  4.78904128e-01,\n         1.00775802e+00, -3.75754714e-01, -3.46514732e-01,\n         4.15985525e-01, -5.44880807e-01, -9.42038357e-01,\n         1.97300661e+00,  2.82769762e-02, -1.40033877e+00,\n        -2.54035258e+00, -9.99269664e-01,  1.16532266e+00,\n        -2.02652240e+00,  6.17954135e-01, -6.49945140e-01,\n        -1.16904177e-01, -7.31663406e-02, -1.00932419e+00,\n        -3.43702316e-01, -1.56791341e+00,  4.09748524e-01,\n        -5.03984809e-01,  6.24953330e-01, -2.81937301e-01,\n        -1.08530305e-01, -1.35634816e+00,  3.33662448e-03,\n        -9.27109241e-01,  1.30734682e+00,  1.90175772e+00,\n         1.16778421e+00, -4.21573043e-01,  6.39304817e-01,\n        -2.48146981e-01, -1.34081280e+00,  6.63784504e-01,\n         1.49230313e+00, -7.91006386e-02, -1.34781802e+00,\n        -1.97537959e-01,  3.51060987e-01, -3.78253132e-01,\n        -5.12566507e-01, -1.21763778e+00,  4.87963296e-02,\n        -4.28180426e-01, -4.74215865e-01,  1.31409571e-01,\n        -2.33126211e+00, -1.35440087e+00, -7.91953683e-01,\n         2.83002996e+00,  1.11304104e+00,  3.99453014e-01,\n         4.42219645e-01,  6.20376289e-01, -5.48857987e-01,\n         6.98266029e-01,  7.37755969e-02,  1.10690284e+00,\n        -7.30244517e-02,  6.98024809e-01,  1.13259113e+00,\n         1.08666134e+00, -1.31575197e-01,  6.07384257e-02,\n        -1.19555330e+00,  3.64213437e-01, -5.70270360e-01,\n         5.02369761e-01,  1.92347217e+00, -8.68119657e-01,\n         5.19087255e-01, -3.59749526e-01, -5.10488749e-01,\n        -2.19268560e+00, -5.09948432e-01,  6.51134074e-01,\n        -1.73730695e+00, -2.52750784e-01, -1.51861560e+00,\n         9.09959078e-01,  1.23570263e+00,  7.53858149e-01,\n         2.94908695e-02,  9.44975674e-01,  1.06093645e+00,\n         2.34311000e-01,  1.22034669e-01, -1.53849471e+00,\n        -2.01079869e+00,  8.50895524e-01,  6.07847691e-01,\n        -2.24608612e+00, -1.13475573e+00,  1.09345533e-01,\n         3.98711324e-01,  1.30968702e+00,  1.46482193e+00,\n        -4.82711375e-01,  9.86348212e-01, -9.56227422e-01,\n         4.24644470e-01, -5.31385541e-01,  3.79579961e-01,\n         4.17282552e-01, -1.72645882e-01,  4.59493488e-01,\n        -1.01957631e+00, -9.20312464e-01, -1.10895145e+00,\n        -7.01783895e-01, -1.19267786e+00, -1.75224151e-02,\n         3.93496841e-01,  8.23328316e-01, -1.64897561e+00,\n         1.47111654e+00,  1.21523905e+00, -1.59713531e+00,\n        -2.54201603e+00, -2.10471135e-02, -1.50686955e+00,\n        -2.88041651e-01,  2.36874744e-01,  1.23177147e+00,\n        -7.60713637e-01,  7.64155090e-02, -1.47013593e+00,\n         5.07555902e-01, -1.06851065e+00,  1.22241092e+00,\n        -5.69646895e-01,  2.43326932e-01, -2.59501368e-01,\n         3.08713894e-02, -1.66259810e-01, -3.28252107e-01,\n        -2.98914075e-01, -3.14603281e+00, -1.36478698e+00,\n         5.95639408e-01,  5.21296859e-01, -2.07427716e+00,\n        -7.58782178e-02,  5.16943395e-01, -1.28900958e-03,\n         1.26635623e+00, -1.62203479e+00,  1.55865550e+00,\n         1.36080444e+00, -1.59578586e+00,  1.49836445e+00,\n         3.43459636e-01, -7.72870719e-01,  7.96870351e-01,\n        -1.47210562e+00, -1.18892050e+00,  1.02298990e-01,\n         1.64208579e+00, -4.66623753e-01, -2.93534845e-01,\n        -7.05007792e-01,  1.14316785e+00,  7.94808626e-01,\n         1.90661860e+00,  6.76017761e-01,  1.60773468e+00,\n        -3.46284717e-01, -9.55185711e-01, -9.99563754e-01]], dtype=float32)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_query_output(\"el perro es un animal\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T02:21:26.158719200Z",
     "start_time": "2024-01-12T02:21:26.035617500Z"
    }
   },
   "id": "a4854c19dab677c0"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DPRContextEncoder were not initialized from the model checkpoint at IIC/dpr-spanish-passage_encoder-allqa-base and are newly initialized: ['bert_model.encoder.layer.5.output.LayerNorm.weight', 'bert_model.encoder.layer.11.output.LayerNorm.bias', 'bert_model.encoder.layer.5.attention.output.dense.weight', 'bert_model.encoder.layer.7.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.6.attention.self.query.weight', 'bert_model.encoder.layer.7.intermediate.dense.bias', 'bert_model.encoder.layer.4.output.LayerNorm.weight', 'bert_model.encoder.layer.10.attention.self.query.weight', 'bert_model.encoder.layer.9.intermediate.dense.bias', 'bert_model.encoder.layer.9.attention.self.key.bias', 'bert_model.encoder.layer.0.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.11.attention.self.query.bias', 'bert_model.encoder.layer.11.output.dense.weight', 'bert_model.encoder.layer.6.attention.output.dense.weight', 'bert_model.encoder.layer.9.attention.self.value.bias', 'bert_model.encoder.layer.7.output.dense.weight', 'bert_model.encoder.layer.11.attention.self.key.weight', 'bert_model.encoder.layer.6.intermediate.dense.weight', 'bert_model.encoder.layer.9.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.8.attention.output.dense.weight', 'bert_model.encoder.layer.5.attention.self.query.bias', 'bert_model.encoder.layer.6.attention.self.value.weight', 'bert_model.encoder.layer.6.attention.self.key.weight', 'bert_model.encoder.layer.1.output.dense.weight', 'bert_model.encoder.layer.0.attention.self.key.weight', 'bert_model.encoder.layer.2.output.LayerNorm.weight', 'bert_model.encoder.layer.10.output.LayerNorm.weight', 'bert_model.encoder.layer.2.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.1.attention.output.dense.bias', 'bert_model.encoder.layer.0.output.LayerNorm.bias', 'bert_model.embeddings.word_embeddings.weight', 'bert_model.encoder.layer.7.output.LayerNorm.weight', 'bert_model.encoder.layer.1.attention.self.key.weight', 'bert_model.encoder.layer.9.attention.self.query.bias', 'bert_model.encoder.layer.1.attention.output.dense.weight', 'bert_model.encoder.layer.5.intermediate.dense.weight', 'bert_model.encoder.layer.11.attention.output.dense.bias', 'bert_model.embeddings.position_embeddings.weight', 'bert_model.encoder.layer.0.output.dense.weight', 'bert_model.encoder.layer.7.attention.self.query.weight', 'bert_model.encoder.layer.1.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.2.attention.output.dense.bias', 'bert_model.encoder.layer.9.attention.self.query.weight', 'bert_model.encoder.layer.11.attention.self.key.bias', 'bert_model.encoder.layer.4.output.dense.bias', 'bert_model.encoder.layer.7.intermediate.dense.weight', 'bert_model.encoder.layer.4.attention.self.value.weight', 'bert_model.encoder.layer.11.attention.self.query.weight', 'bert_model.encoder.layer.7.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.4.attention.self.query.bias', 'bert_model.encoder.layer.10.attention.self.key.weight', 'bert_model.encoder.layer.3.attention.output.dense.weight', 'bert_model.encoder.layer.7.attention.self.key.bias', 'bert_model.encoder.layer.5.attention.self.value.weight', 'bert_model.encoder.layer.8.attention.self.key.bias', 'bert_model.encoder.layer.3.attention.output.dense.bias', 'bert_model.encoder.layer.5.attention.self.value.bias', 'bert_model.encoder.layer.5.attention.output.dense.bias', 'bert_model.encoder.layer.1.output.LayerNorm.bias', 'bert_model.encoder.layer.7.attention.self.value.weight', 'bert_model.encoder.layer.5.output.LayerNorm.bias', 'bert_model.encoder.layer.9.attention.output.dense.bias', 'bert_model.encoder.layer.5.attention.self.key.weight', 'bert_model.encoder.layer.4.attention.self.value.bias', 'bert_model.encoder.layer.2.attention.self.key.bias', 'bert_model.encoder.layer.0.attention.self.value.bias', 'bert_model.encoder.layer.4.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.3.attention.self.value.bias', 'bert_model.encoder.layer.9.output.LayerNorm.weight', 'bert_model.encoder.layer.8.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.2.output.dense.bias', 'bert_model.encoder.layer.3.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.9.output.dense.weight', 'bert_model.encoder.layer.9.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.0.attention.self.key.bias', 'bert_model.encoder.layer.1.attention.self.query.weight', 'bert_model.encoder.layer.1.attention.self.key.bias', 'bert_model.encoder.layer.4.attention.self.query.weight', 'bert_model.encoder.layer.11.output.dense.bias', 'bert_model.encoder.layer.9.attention.self.value.weight', 'bert_model.encoder.layer.10.attention.self.value.weight', 'bert_model.encoder.layer.0.intermediate.dense.bias', 'bert_model.encoder.layer.9.attention.self.key.weight', 'bert_model.encoder.layer.0.attention.output.dense.weight', 'bert_model.encoder.layer.4.intermediate.dense.weight', 'bert_model.encoder.layer.4.attention.self.key.bias', 'bert_model.encoder.layer.10.attention.self.query.bias', 'bert_model.encoder.layer.10.intermediate.dense.weight', 'bert_model.encoder.layer.8.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.0.output.dense.bias', 'bert_model.encoder.layer.7.attention.output.dense.weight', 'bert_model.encoder.layer.6.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.3.attention.self.value.weight', 'bert_model.encoder.layer.7.output.dense.bias', 'bert_model.encoder.layer.2.intermediate.dense.bias', 'bert_model.encoder.layer.8.output.dense.bias', 'bert_model.encoder.layer.3.attention.self.query.weight', 'bert_model.encoder.layer.5.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.0.attention.self.value.weight', 'bert_model.encoder.layer.6.attention.self.query.bias', 'bert_model.encoder.layer.2.output.dense.weight', 'bert_model.encoder.layer.0.intermediate.dense.weight', 'bert_model.encoder.layer.8.attention.self.value.weight', 'bert_model.encoder.layer.3.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.4.attention.output.dense.weight', 'bert_model.encoder.layer.8.attention.self.value.bias', 'bert_model.encoder.layer.8.attention.self.query.weight', 'bert_model.encoder.layer.11.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.3.attention.self.key.weight', 'bert_model.encoder.layer.10.attention.self.key.bias', 'bert_model.encoder.layer.9.intermediate.dense.weight', 'bert_model.encoder.layer.1.attention.self.value.bias', 'bert_model.encoder.layer.3.intermediate.dense.bias', 'bert_model.encoder.layer.6.attention.self.key.bias', 'bert_model.encoder.layer.5.attention.self.key.bias', 'bert_model.encoder.layer.1.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.1.output.dense.bias', 'bert_model.encoder.layer.3.output.LayerNorm.bias', 'bert_model.encoder.layer.11.output.LayerNorm.weight', 'bert_model.encoder.layer.6.attention.self.value.bias', 'bert_model.encoder.layer.10.output.LayerNorm.bias', 'bert_model.encoder.layer.6.output.dense.bias', 'bert_model.encoder.layer.10.attention.self.value.bias', 'bert_model.encoder.layer.2.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.4.output.dense.weight', 'bert_model.encoder.layer.1.output.LayerNorm.weight', 'bert_model.encoder.layer.2.intermediate.dense.weight', 'bert_model.encoder.layer.5.attention.self.query.weight', 'bert_model.encoder.layer.7.attention.self.query.bias', 'bert_model.encoder.layer.5.output.dense.bias', 'bert_model.encoder.layer.3.output.dense.weight', 'bert_model.encoder.layer.0.attention.self.query.bias', 'bert_model.encoder.layer.5.output.dense.weight', 'bert_model.encoder.layer.8.attention.self.query.bias', 'bert_model.encoder.layer.0.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.8.output.LayerNorm.bias', 'bert_model.encoder.layer.10.intermediate.dense.bias', 'bert_model.encoder.layer.10.output.dense.bias', 'bert_model.encoder.layer.11.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.11.attention.self.value.weight', 'bert_model.encoder.layer.9.output.dense.bias', 'bert_model.encoder.layer.11.attention.self.value.bias', 'bert_model.encoder.layer.1.attention.self.value.weight', 'bert_model.encoder.layer.3.output.dense.bias', 'bert_model.encoder.layer.2.attention.self.value.bias', 'bert_model.encoder.layer.7.attention.self.value.bias', 'bert_model.encoder.layer.4.intermediate.dense.bias', 'bert_model.encoder.layer.8.intermediate.dense.bias', 'bert_model.encoder.layer.3.output.LayerNorm.weight', 'bert_model.encoder.layer.9.attention.output.dense.weight', 'bert_model.encoder.layer.10.attention.output.dense.weight', 'bert_model.encoder.layer.8.attention.self.key.weight', 'bert_model.encoder.layer.6.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.4.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.10.attention.output.LayerNorm.weight', 'bert_model.encoder.layer.3.attention.self.key.bias', 'bert_model.encoder.layer.10.attention.output.dense.bias', 'bert_model.encoder.layer.2.attention.self.key.weight', 'bert_model.encoder.layer.6.output.dense.weight', 'bert_model.encoder.layer.5.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.1.intermediate.dense.weight', 'bert_model.encoder.layer.7.attention.output.dense.bias', 'bert_model.encoder.layer.3.attention.self.query.bias', 'bert_model.embeddings.LayerNorm.weight', 'bert_model.encoder.layer.4.attention.output.dense.bias', 'bert_model.embeddings.LayerNorm.bias', 'bert_model.encoder.layer.9.output.LayerNorm.bias', 'bert_model.encoder.layer.11.attention.output.dense.weight', 'bert_model.encoder.layer.6.intermediate.dense.bias', 'bert_model.encoder.layer.1.intermediate.dense.bias', 'bert_model.embeddings.token_type_embeddings.weight', 'bert_model.encoder.layer.7.output.LayerNorm.bias', 'bert_model.encoder.layer.0.attention.self.query.weight', 'bert_model.encoder.layer.4.output.LayerNorm.bias', 'bert_model.encoder.layer.0.output.LayerNorm.weight', 'bert_model.encoder.layer.8.attention.output.dense.bias', 'bert_model.encoder.layer.2.attention.self.query.weight', 'bert_model.encoder.layer.3.intermediate.dense.weight', 'bert_model.encoder.layer.8.intermediate.dense.weight', 'bert_model.encoder.layer.8.output.dense.weight', 'bert_model.encoder.layer.11.intermediate.dense.bias', 'bert_model.encoder.layer.2.attention.self.query.bias', 'bert_model.encoder.layer.1.attention.self.query.bias', 'bert_model.encoder.layer.4.attention.self.key.weight', 'bert_model.encoder.layer.6.output.LayerNorm.weight', 'bert_model.encoder.layer.6.attention.output.dense.bias', 'bert_model.encoder.layer.5.intermediate.dense.bias', 'bert_model.encoder.layer.8.output.LayerNorm.weight', 'bert_model.encoder.layer.2.attention.output.dense.weight', 'bert_model.encoder.layer.6.output.LayerNorm.bias', 'bert_model.encoder.layer.11.intermediate.dense.weight', 'bert_model.encoder.layer.0.attention.output.dense.bias', 'bert_model.encoder.layer.10.output.dense.weight', 'bert_model.encoder.layer.2.output.LayerNorm.bias', 'bert_model.encoder.layer.2.attention.self.value.weight', 'bert_model.encoder.layer.10.attention.output.LayerNorm.bias', 'bert_model.encoder.layer.7.attention.self.key.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "context_encoder_model = DPRContextEncoder.from_pretrained(\"IIC/dpr-spanish-passage_encoder-allqa-base\")\n",
    "context_encoder_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"IIC/dpr-spanish-passage_encoder-allqa-base\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T01:59:45.994937300Z",
     "start_time": "2024-01-12T01:59:43.758629500Z"
    }
   },
   "id": "7244ba1b64317cd0"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "input_context = '''programa : diplomado en banca y transformación digital \n",
    "certificado academico : diplomado en banca y transformación digital\n",
    "unidad de ejecucion : Centro de Estudios de Posgrado e Investigación\n",
    "Facultad de Contaduría Pública y Ciencias Financieras\n",
    "sede :  SUCRE: \n",
    "CEPI: Calle Aniceto Arce No. 46, zona central.\n",
    "Telf. y Fax: (591) 4 6440887\n",
    "Facultad de Contaduría Pública y Ciencias Financieras: Calle Grau Nº 149  .\n",
    "TLF: (591) 4 6434025 - (591) 4 6452320\n",
    "costo total : Bs \t3.600, 00\n",
    "modalidad de ejecucion : virtual\n",
    "'''\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T02:00:56.086733700Z",
     "start_time": "2024-01-12T02:00:56.055200800Z"
    }
   },
   "id": "791d66b51be174de"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def model_ctxt_output(text):\n",
    "    context_token = context_encoder_tokenizer(input_context, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    output_context = context_encoder_model(context_token).pooler_output\n",
    "    return output_context.detach().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T02:22:22.795362800Z",
     "start_time": "2024-01-12T02:22:22.776845600Z"
    }
   },
   "id": "11f69fd472390182"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1.14829220e-01, -3.91656518e-01,  1.09574795e+00,\n         2.09859443e+00, -9.66568351e-01, -3.78206670e-01,\n         4.35817614e-02, -1.21820249e-01,  5.17445445e-01,\n        -8.74035358e-01,  3.73031870e-02, -4.24682796e-01,\n         1.36622381e+00,  7.19356418e-01, -1.05592012e-01,\n         7.49281943e-01, -2.57143408e-01,  5.38518548e-01,\n        -1.11609781e+00, -5.50528109e-01, -8.79833937e-01,\n         7.92579114e-01, -1.88262618e+00, -9.85792518e-01,\n         1.03639710e+00, -2.60540456e-01, -1.02406323e+00,\n        -1.64425516e+00,  5.53443611e-01, -1.01424766e+00,\n        -1.81939662e-01, -1.57043350e+00,  6.11576617e-01,\n         9.41590816e-02, -1.08460784e+00, -9.87856388e-01,\n         2.87408428e-03,  1.33704567e+00,  2.22832227e+00,\n        -5.50818816e-02,  4.79457140e-01, -1.09226358e+00,\n         2.87283152e-01, -6.20542645e-01, -1.58212200e-01,\n        -1.17285833e-01, -3.54650199e-01,  1.27832025e-01,\n        -3.63534361e-01, -3.47788893e-02,  3.88755918e-01,\n        -1.54409134e+00, -4.48060870e-01,  8.80434513e-01,\n         1.88130260e+00, -8.88896644e-01, -1.40218890e+00,\n         5.91324508e-01,  1.80875018e-01,  3.86536330e-01,\n         2.96574831e-01,  1.71258116e+00,  8.72814476e-01,\n        -2.89526045e-01, -2.12075615e+00, -2.62840062e-01,\n         7.31325746e-01, -6.36389792e-01, -1.17164947e-01,\n         1.13525355e+00,  1.35597438e-01, -1.56146258e-01,\n         2.19489169e+00, -2.01148105e+00,  4.82762456e-01,\n         2.10615110e+00, -1.91522563e+00, -6.84972763e-01,\n        -1.01732528e+00,  5.41250765e-01, -7.00642169e-01,\n        -7.29158878e-01, -1.02796149e+00, -6.76313162e-01,\n        -1.04610157e+00, -1.43042052e+00, -1.92284274e+00,\n        -2.28324890e-01,  6.66974902e-01, -1.12287390e+00,\n         2.09010220e+00, -1.33823740e+00, -2.08755881e-01,\n         6.11550152e-01, -1.01698327e+00,  3.80715728e-01,\n        -1.79279402e-01,  3.08306068e-01,  2.36474586e+00,\n        -3.39458376e-01,  1.87073022e-01, -7.18138158e-01,\n         1.02491903e+00, -2.62042135e-01,  7.56678939e-01,\n        -4.81895477e-01,  6.32466376e-01, -1.64909706e-01,\n         2.98339784e-01,  7.60040939e-01,  4.82284784e-01,\n         6.73598886e-01, -3.81407499e-01,  5.72051287e-01,\n         4.63785797e-01,  1.13116682e+00,  1.78888428e+00,\n         1.26289085e-01,  1.34820509e+00, -6.07465841e-02,\n         6.81304395e-01, -1.39046228e+00, -1.11371827e+00,\n        -2.96540231e-01, -1.28905535e-01,  1.06042743e+00,\n        -3.86069357e-01, -5.96009661e-03,  2.21846059e-01,\n         1.45936930e+00,  5.85561812e-01, -2.82126635e-01,\n         1.36071277e+00,  3.61389667e-01, -8.40553582e-01,\n        -6.25767052e-01,  5.50924778e-01, -2.96273065e+00,\n         1.55486202e+00, -2.79722154e-01,  9.84798670e-01,\n        -1.10452771e+00, -1.05405354e+00,  6.15263768e-02,\n         1.31182864e-01,  1.38882136e+00,  1.15169847e+00,\n        -5.28993487e-01, -1.07742059e+00,  7.17670381e-01,\n        -1.01159978e+00, -3.56176406e-01,  5.58009982e-01,\n        -9.85366404e-01, -1.47203594e-01, -6.79456413e-01,\n         1.05586386e+00, -1.37397444e+00,  9.63211119e-01,\n        -1.54435790e+00, -8.31881225e-01, -8.25682104e-01,\n        -2.02481836e-01, -2.17780995e+00,  1.10535157e+00,\n        -1.29558754e+00,  1.21582639e+00,  5.40796518e-02,\n         1.06620395e+00,  4.50985789e-01, -2.17212868e+00,\n         1.42009699e+00,  1.19561583e-01, -6.37693226e-01,\n         1.30191550e-01, -1.19524634e+00,  4.78311718e-01,\n         1.01374531e+00,  7.96923339e-01,  1.05389416e-01,\n        -1.24716842e+00,  8.16282988e-01,  1.65302789e+00,\n        -8.66930783e-01, -1.85907930e-01,  1.79571545e+00,\n         2.48692974e-01,  9.41860676e-01,  1.53476909e-01,\n        -2.39009285e+00,  2.16192544e-01, -1.65067840e+00,\n         1.41723305e-01,  1.63134933e+00, -1.60914493e+00,\n         1.20493519e+00,  4.87587243e-01,  3.06358129e-01,\n         1.46711409e+00,  1.31634939e+00,  2.89387107e-01,\n        -4.72351551e-01,  4.67412740e-01, -3.37898284e-02,\n        -5.60647309e-01,  1.04373777e+00, -1.54048905e-01,\n        -5.63099384e-01,  6.40662313e-01,  5.07893860e-01,\n        -7.75544226e-01,  1.99501574e+00, -3.66399795e-01,\n         7.45281160e-01,  3.86797994e-01,  7.99429975e-03,\n        -1.03945994e+00, -2.73206329e+00,  5.73171914e-01,\n        -4.91150886e-01,  1.00144649e+00,  7.89324880e-01,\n        -2.24285871e-01, -9.26108599e-01,  1.52160212e-01,\n         1.19295165e-01, -8.34157288e-01, -1.56825149e+00,\n         1.16458130e+00, -1.78200543e-01,  1.17947841e+00,\n         1.40145671e+00,  4.14846569e-01, -1.09263885e+00,\n        -2.36252502e-01, -1.09729171e+00,  2.80325979e-01,\n        -6.92658484e-01,  3.49313736e-01,  1.39055669e+00,\n        -6.37274563e-01,  6.06868744e-01,  3.02170062e+00,\n         5.45186281e-01, -2.21637771e-01,  2.17545941e-01,\n         1.42090988e+00, -1.03300464e+00, -1.44375861e+00,\n         1.34985459e+00,  1.35276377e+00, -4.96727049e-01,\n        -5.05205393e-01, -4.31441396e-01, -6.83485746e-01,\n         6.58431277e-02, -5.34518138e-02, -3.88844818e-01,\n         8.00684631e-01,  9.31089699e-01, -1.22042143e+00,\n         9.13911283e-01, -4.77849633e-01,  5.31673372e-01,\n         1.65336683e-01, -4.05515254e-01, -3.17227411e+00,\n        -1.95139468e-01,  8.21941137e-01, -7.60006011e-01,\n        -1.72093332e-01, -1.40773308e+00, -1.27992892e+00,\n         9.27449524e-01,  2.75485420e+00, -1.32486975e+00,\n         8.33809078e-02, -6.10120416e-01, -1.72923520e-01,\n         1.87406361e+00, -1.82899982e-01, -9.90208030e-01,\n        -2.32594895e+00, -1.26136333e-01, -9.18068111e-01,\n        -1.42740798e+00, -2.99066246e-01, -2.15874150e-01,\n         1.08535421e+00,  4.48161870e-01,  1.16508687e+00,\n         1.21802258e+00,  4.06709850e-01,  7.23616958e-01,\n         1.49425137e+00, -4.77081388e-01, -1.01939595e+00,\n         1.41108692e+00, -3.72341722e-01,  1.68321943e+00,\n         1.67756081e+00,  2.97671016e-02,  1.40413785e+00,\n        -5.34199953e-01, -7.79269338e-01,  1.62043065e-01,\n        -3.59799683e-01,  1.32342964e-01,  7.88733065e-01,\n         7.66889095e-01, -7.61352360e-01, -3.28207463e-01,\n         7.63802886e-01,  4.58374113e-01,  6.81813240e-01,\n        -8.57162382e-03, -1.31248581e+00,  4.07948732e-01,\n        -2.45260167e+00,  1.93617570e+00, -5.62972069e-01,\n        -1.22550881e+00, -4.83451277e-01, -1.82832122e+00,\n         2.72730350e-01,  1.77120835e-01,  9.35097098e-01,\n         6.86140835e-01,  7.24686623e-01,  2.24126965e-01,\n        -8.62284064e-01, -3.32807392e-01,  5.86998425e-02,\n        -8.56341302e-01,  6.53771996e-01, -1.24571121e+00,\n        -3.50378901e-01, -4.35782790e-01,  2.03745365e+00,\n         1.43443310e+00,  3.74642074e-01,  2.09256038e-01,\n         7.47392893e-01, -1.79461360e+00, -3.42581093e-01,\n        -1.01255810e+00,  1.89132667e+00, -1.89554667e+00,\n         1.93737075e-01,  9.41052020e-01,  1.32414818e+00,\n        -7.37011850e-01,  1.34046376e+00, -1.36816490e+00,\n         5.40123023e-02,  6.73850894e-01, -4.36032832e-01,\n         4.78926003e-01, -9.13041353e-01, -1.22377741e+00,\n        -2.73344040e-01, -2.14206338e-01,  1.12808442e+00,\n         6.68768883e-02, -9.34850097e-01,  1.68055344e+00,\n         7.84077764e-01,  1.39896584e+00, -1.23805463e+00,\n        -6.30864263e-01,  1.54391623e+00, -1.97010827e+00,\n         1.43450725e+00,  2.27165890e+00,  6.53536320e-01,\n        -4.43609595e-01, -6.95474386e-01, -1.75753391e+00,\n        -5.95556915e-01, -7.17368603e-01, -9.84761477e-01,\n         5.67584515e-01, -6.56062365e-01, -6.54689491e-01,\n        -7.68058226e-02, -1.09561808e-01,  2.67954737e-01,\n        -7.27189600e-01, -5.01495838e-01, -1.85426295e-01,\n        -5.09472072e-01,  1.44341618e-01, -3.38701546e-01,\n        -3.28747392e-01, -6.10550642e-01,  2.38428116e+00,\n        -1.44686830e+00,  1.05149150e+00, -2.08223248e+00,\n        -7.71880329e-01, -7.20060945e-01, -1.06553888e+00,\n        -5.00692904e-01,  4.73836720e-01, -2.03765130e+00,\n        -1.88987076e+00,  1.16956496e+00, -7.54909217e-01,\n        -2.25065663e-01, -1.64250708e+00, -9.90592182e-01,\n        -5.03992558e-01, -9.42792594e-01, -4.38184440e-02,\n         1.89826715e+00,  7.04445779e-01,  8.65914702e-01,\n         9.82240915e-01, -1.32084742e-01, -7.31530488e-01,\n         8.56297970e-01, -5.46064258e-01, -4.92896199e-01,\n         2.09190941e+00,  5.92155643e-02,  4.35031623e-01,\n        -1.05694190e-01, -1.24244583e+00, -1.88236669e-01,\n         8.75887811e-01,  1.66084492e+00, -1.16015935e+00,\n         3.40173542e-01,  1.43064463e+00, -4.39001590e-01,\n        -7.35590339e-01,  1.56850111e+00, -2.23575562e-01,\n        -1.08623065e-01,  2.53859852e-02, -9.19386625e-01,\n         2.85988092e-01,  1.55463147e+00,  1.67606652e+00,\n         1.41625154e+00, -1.08326447e+00,  2.01916480e+00,\n        -5.02778888e-01, -3.47837538e-01, -1.62943542e-01,\n        -1.55606437e+00, -1.79673314e-01,  8.13539147e-01,\n         2.23960906e-01,  2.30700359e-01,  2.11584258e+00,\n         8.69595885e-01,  4.67987359e-02, -1.17021787e+00,\n        -1.06137228e+00,  1.73831880e-01, -4.83177096e-01,\n        -4.76503342e-01, -3.77389938e-01,  2.21942449e+00,\n        -1.52051699e+00,  4.02851790e-01,  3.23301435e-01,\n         3.84284645e-01,  1.36605644e+00, -7.59904504e-01,\n         3.06430012e-01, -6.75549626e-01, -6.44106805e-01,\n        -1.34716511e+00,  1.85117394e-01,  8.99573088e-01,\n         6.71938732e-02, -2.66091615e-01,  5.77292025e-01,\n         1.45090020e+00,  9.67468917e-01, -6.17291749e-01,\n         5.65936446e-01, -1.86653018e-01, -1.03818071e+00,\n         7.90866673e-01,  8.28629971e-01, -1.51508361e-01,\n        -6.22458518e-01, -3.55298430e-01,  8.10050845e-01,\n         1.49254084e+00, -1.55110419e+00,  4.00655925e-01,\n        -2.04413366e-02, -1.90622266e-02, -8.48143756e-01,\n         8.42720568e-01,  2.16545075e-01, -1.60042331e-01,\n         3.26733559e-01,  4.70752388e-01,  7.23653018e-01,\n        -1.15264976e+00, -1.05927908e+00,  1.93359780e+00,\n         1.45194829e+00,  1.50050795e+00, -1.71845019e-01,\n        -1.51324201e+00,  1.10300517e+00, -2.13480306e+00,\n         9.85381484e-01,  9.07573178e-02,  6.29554451e-01,\n         1.10053015e+00, -2.65533984e-01,  1.64866078e+00,\n        -1.10991049e+00, -1.24599242e+00, -5.53007543e-01,\n         1.40133727e+00, -7.83976734e-01,  1.04212558e+00,\n        -1.45684211e-02, -2.72849858e-01, -6.94329560e-01,\n        -1.61987662e+00,  1.18545756e-01,  1.14039886e+00,\n        -4.68968332e-01, -1.86278105e+00, -1.06447303e+00,\n         1.80905187e+00,  4.01700914e-01,  5.26524603e-01,\n        -1.22189546e+00,  1.00843298e+00,  4.47194368e-01,\n        -3.85097474e-01,  2.08936095e-01,  2.22889677e-01,\n        -3.36435437e-01,  5.21480620e-01,  9.17223036e-01,\n        -1.39618361e+00,  1.24731779e-01, -6.63219094e-02,\n        -5.55916786e-01,  1.53166234e-01,  8.27483118e-01,\n        -1.40099391e-01,  1.45876735e-01,  1.69845724e+00,\n        -1.08436286e-01, -2.78916121e-01,  6.81795895e-01,\n        -1.50734687e+00,  1.64070821e+00, -1.64687526e+00,\n         5.48845939e-02,  6.21614516e-01, -1.04834867e+00,\n         3.40861142e-01,  1.17950284e+00,  3.64208221e-01,\n         1.85815543e-01, -8.89366269e-01,  1.30149674e+00,\n        -4.43720579e-01, -4.48255271e-01,  2.18610704e-01,\n        -1.05207396e+00, -5.43980837e-01,  2.25869656e-01,\n         1.23438787e+00,  8.36139679e-01, -9.57057253e-02,\n         1.26915257e-02,  8.00009906e-01, -1.33663487e+00,\n        -2.46399362e-03, -4.88694847e-01, -5.85584521e-01,\n         2.00299576e-01, -7.63920963e-01,  4.30701196e-01,\n        -3.36202770e-01, -3.61967921e-01,  2.10447812e+00,\n         1.18931130e-01, -4.38195616e-01,  3.41747135e-01,\n         1.82285726e+00,  6.46660686e-01, -1.05981410e+00,\n        -9.51926410e-01,  2.58842051e-01, -2.34594017e-01,\n         4.11093831e-01, -2.73499399e-01,  4.04105097e-01,\n        -3.08958977e-01,  8.34436417e-01, -5.46969235e-01,\n         9.31257844e-01, -1.06066537e+00, -3.58530521e-01,\n        -1.67905354e+00, -2.22695661e+00, -8.16122293e-01,\n        -1.23474944e+00,  1.85437873e-01,  1.35402644e+00,\n         3.66701692e-01,  2.35755399e-01, -1.87303054e+00,\n         5.96489787e-01,  4.76802230e-01, -5.23415208e-02,\n         3.33802223e-01,  5.38672090e-01, -4.74987179e-01,\n        -1.06890845e+00,  1.05198741e+00, -9.94505823e-01,\n        -1.25492191e+00, -3.67980033e-01, -1.09431863e+00,\n         2.59161741e-01, -3.14486295e-01,  4.46290702e-01,\n         1.27001929e+00,  9.86648619e-01,  1.59832323e+00,\n         2.56365929e-02,  5.89932621e-01, -1.26870558e-01,\n        -5.44710606e-02, -5.63666597e-02,  2.33881760e+00,\n        -8.39338601e-02, -7.05195427e-01, -1.37800527e+00,\n        -5.81838548e-01, -3.25374097e-01, -9.03741539e-01,\n         9.85416889e-01,  5.55590093e-01,  5.66028178e-01,\n         4.07550395e-01,  1.49346840e+00, -1.45295471e-01,\n        -1.93577945e+00, -6.26273990e-01, -6.43767953e-01,\n         1.11203563e+00, -7.74202228e-01,  1.73588291e-01,\n         5.01588620e-02, -3.22537243e-01,  1.20326960e+00,\n        -1.45937276e+00, -1.02027225e+00,  4.60043699e-01,\n        -2.15930045e-01, -8.93398762e-01, -1.44321454e+00,\n         1.76384389e+00,  5.99587895e-02, -1.60495019e+00,\n         5.15546918e-01, -1.70589411e+00, -1.48392463e+00,\n         4.49101776e-01, -1.59805286e+00,  3.79463345e-01,\n        -1.24278510e+00, -1.85282922e+00,  7.16382027e-01,\n        -1.31407785e+00,  1.53504443e+00, -1.59261569e-01,\n         9.35426891e-01,  6.31731033e-01,  2.17696953e+00,\n        -4.08397056e-04,  1.65641636e-01, -8.81833434e-01,\n         1.23138058e+00,  7.27479577e-01,  6.18509173e-01,\n        -5.78461349e-01, -1.95089817e-01,  8.07820797e-01,\n         2.92409807e-01,  1.73149798e-02,  7.52642155e-01,\n         1.41048419e+00,  5.63342161e-02,  1.27338126e-01,\n         1.49096715e+00, -4.29215699e-01, -3.64614785e-01,\n        -1.55549794e-01,  4.31539059e-01,  1.41243905e-01,\n        -2.47837782e+00, -1.07960284e+00,  3.92047048e-01,\n        -2.47290745e-01, -3.58650088e-01,  4.80876446e-01,\n        -5.31898558e-01,  2.85995603e-01,  1.24769151e+00,\n         3.71120751e-01,  1.35051966e+00,  1.66915643e+00,\n        -3.15086722e-01, -7.29261041e-01, -2.40002707e-01,\n        -1.05883813e+00, -4.10286158e-01, -7.89025724e-01,\n        -7.32886910e-01,  1.64418590e+00, -1.36830628e+00,\n         6.96409494e-02, -2.02416968e+00,  1.11661661e+00,\n         1.89287615e+00,  1.06219172e+00, -3.59527260e-01,\n        -1.10690427e+00,  1.04900444e+00, -7.17569351e-01,\n        -2.98881698e-02,  4.55901742e-01, -6.15520835e-01,\n         3.36513132e-01, -5.58753312e-01, -1.46727502e+00,\n        -7.82563806e-01,  9.55539346e-01, -1.46558666e+00,\n        -2.47477606e-01, -8.51412117e-01, -8.75139892e-01,\n         1.95160508e+00, -6.59799218e-01, -4.13400948e-01,\n        -9.61765885e-01, -6.95848644e-01, -1.32933772e+00,\n        -2.15628281e-01,  1.11428952e+00,  2.37275505e+00,\n         2.85791427e-01,  5.77014923e-01, -2.18125582e-01,\n        -4.09068108e-01, -3.46045643e-01,  2.73936570e-01,\n        -2.64406949e-01, -1.29092824e+00,  1.81485999e+00]], dtype=float32)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ctxt_output(input_context)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T02:22:23.642472900Z",
     "start_time": "2024-01-12T02:22:23.328819800Z"
    }
   },
   "id": "23c81ca532c2311a"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T02:11:29.176396100Z",
     "start_time": "2024-01-12T02:11:29.144842300Z"
    }
   },
   "id": "c39eb7838641e68a"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "query = model_query_output(\"a cuanto cuesta el curso de diplomado en banca y transformacion digital\")\n",
    "ctxt = model_ctxt_output(input_context)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T02:24:47.838662900Z",
     "start_time": "2024-01-12T02:24:47.550100400Z"
    }
   },
   "id": "2083a4aed0dcde9d"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.02152175]], dtype=float32)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(query, ctxt) # el perro es un animal"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T02:22:48.607891300Z",
     "start_time": "2024-01-12T02:22:48.596722300Z"
    }
   },
   "id": "fd496c3b814b5c21"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.02406216]], dtype=float32)"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(query, ctxt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T02:24:12.571711900Z",
     "start_time": "2024-01-12T02:24:12.555926800Z"
    }
   },
   "id": "43ee11b295b573da"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.02652429]], dtype=float32)"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(query, ctxt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T02:24:53.596656100Z",
     "start_time": "2024-01-12T02:24:53.577020300Z"
    }
   },
   "id": "44abc33884bea2b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d344141394cc657b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
